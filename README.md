<div align="center"> 

<a href="https://github.com/OD-FinLLM/" target="_blank"><img src="./web_demo/assets/logo.webp" alt="OD-FinLLM" style="width: 80%; min-width: 350px; display: block; margin: auto;"></a>

[![Maintainer](https://img.shields.io/badge/Maintainer-Firefly-blue.svg "Maintainer")](https://github.com/Firefly-Tech) [![Platform](https://img.shields.io/badge/Platform-Linux%20%7C%20Windows%20%7C%20MacOS-orange.svg "Platform")](./) [![Framework](https://img.shields.io/badge/Framework-Huggingface%20Transformers-lightgrey.svg "Framework")](./)

[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://github.com/OD-FinLLM/LICENSE) [![Python Version](https://img.shields.io/badge/Python-3.9%2B-brightgreen.svg "Python Version")](./)

[![Typing SVG](https://readme-typing-svg.demolab.com?font=Roboto+Mono&pause=100&color=00BFFF&center=true&vCenter=true&width=600&lines=Powering+OD-FinLLM+with+Huggingface%20Transformers)](https://git.io/typing-svg)

</div>


# OD-FinLLM (On-Device Financial Large Language Model)
### OD-FinLLM: A Specialized Model for Chinese Financial Knowledge, Developed by Firefly

OD-FinLLM is a refined model derived from the LLaMA series, with specific enhancements for Chinese financial knowledge. This model is built by fine-tuning LLaMA using a specialized instruction dataset created from publicly available Chinese financial Q&A data and additional web-scraped financial information, leading to superior performance in financial-related queries.


## ğŸ¥ Demo
The following is a demonstration video of OD-FinLLM, demonstrating the application performance of the model in Chinese financial Q&A scenarios. You can watch the embedded video directly here:

<video width="80%" controls> <source src="./web_demo/assets/OD-FinLLM.mp4" type="video/mp4"> Your browser does not support video playback. </video>

## ğŸ”„ Latest Updates

- **[2024/08/24]**: Released a fine-tuned model based on LLaMA and web demo codebase.

## ğŸš€ Quick Installation

Start by installing the necessary dependencies. It is recommended to use Python version 3.9 or above.

```bash
pip install -r requirements.txt
```

### ğŸ¤– Model Availability

You can download the models' weights via Huggingface. The file structure is as follows:

| Model Download Link | Type | Base Model | Training Data | Sequence Length | Version |
|---------------------|------|------------|---------------|-----------------|---------|
| [OD-FinLLM-LLaMA-7B](https://huggingface.co/FireFlyCourageous/OD-FinLLM-LLaMA-7B) | Chinese Financial Q&A Fine-Tuned Model | meta-llama/Llama-2-7b-chat-hf | Instruction Data | 512 | V1.0 |
| [OD-FinLLM-LLaMA-8B](https://huggingface.co/FireFlyCourageous/OD-FinLLM-LLaMA-8B) | Chinese Financial Q&A Fine-Tuned Model | meta-llama/Meta-Llama-3.1-8B-Instruct | Instruction Data | 512 | V1.0 |

### Inference Process


To run inference:

```bash
cd OD-FinLLM
sh run.sh
```


### Web Demo Process

To run web demo:

```bash
python web_demo.py
```

### ğŸ“Š Data Construction

Earlier versions of OD-FinLLM utilized **publicly available and scraped Chinese financial Q&A data**, covering topics such as insurance, wealth management, stocks, funds, loans, credit cards, social security, and more.

Example of instruction data:

```
Question: æ‹›å•†é“¶è¡Œeæ‹›è´·é€¾æœŸäº†æ€ä¹ˆåŠï¼Ÿ

Answer: å¦‚æœä½ çš„æ‹›å•†é“¶è¡Œeæ‹›è´·è´·æ¬¾å·²ç»é€¾æœŸäº†ï¼Œä½ åº”è¯¥ç«‹å³è”ç³»æ‹›å•†é“¶è¡Œå®¢æœï¼Œå°½å¿«å®‰æ’è¿˜æ¬¾ã€‚é€¾æœŸè¿˜æ¬¾ä¼šå¯¹ä½ çš„ä¿¡ç”¨é€ æˆä¸è‰¯å½±å“ï¼Œå¹¶å¯èƒ½ä¼šäº§ç”Ÿé¢å¤–çš„è´¹ç”¨ã€‚å¦‚æœä½ æœ‰å›°éš¾è¿˜æ¬¾ï¼Œå¯ä»¥å‘æ‹›å•†é“¶è¡Œå’¨è¯¢æœ‰å…³å»¶æœŸè¿˜æ¬¾æˆ–å…¶ä»–è§£å†³æ–¹æ¡ˆçš„ä¿¡æ¯ã€‚æ‹›å•†é“¶è¡Œå¯èƒ½ä¼šæä¾›å¸®åŠ©ï¼ŒåŒ…æ‹¬å»¶æœŸè¿˜æ¬¾æˆ–åˆ†æœŸè¿˜æ¬¾ã€‚ä½†æ˜¯ï¼Œå»¶æœŸè¿˜æ¬¾æˆ–åˆ†æœŸè¿˜æ¬¾ä¹Ÿä¼šäº§ç”Ÿé¢å¤–è´¹ç”¨ï¼Œå› æ­¤åº”è°¨æ…è€ƒè™‘ã€‚åœ¨è”ç³»æ‹›å•†é“¶è¡Œä¹‹å‰ï¼Œå»ºè®®ä½ å‡†å¤‡å¥½ç›¸å…³èµ„æ–™ï¼Œä»¥ä¾¿èƒ½å¤Ÿé¡ºåˆ©è§£å†³é—®é¢˜ã€‚è¿™äº›èµ„æ–™åŒ…æ‹¬ä½ çš„è´·æ¬¾åˆåŒã€è´¦å•ä»¥åŠå…¶ä»–æœ‰å…³ä½ è´·æ¬¾çš„æ–‡ä»¶ã€‚æ€»ä¹‹ï¼Œå¦‚æœä½ çš„æ‹›å•†é“¶è¡Œeæ‹›è´·è´·æ¬¾å·²ç»é€¾æœŸäº†ï¼Œåº”å°½å¿«è”ç³»é“¶è¡Œå¹¶å®‰æ’è¿˜æ¬¾ï¼Œä»¥é¿å…é€ æˆæ›´å¤§çš„é—®é¢˜ã€‚
```

To improve data quality and expand the scope, we are currently optimizing and expanding the Chinese financial knowledge base using GPT-3.5/4.0 APIs. The goal is to construct a **diverse and comprehensive instruction dataset** through various prompt formats and multi-task learning to cover a broad range of financial scenarios.



## ğŸƒ Training Specifications

### Hardware Requirements

The current training setup uses four 4090 GPUs, with 10 training epochs. It is recommended to use GPUs with 24GB of memory (e.g., 3090/4090) or higher, adjusting the batch_size according to memory availability.


## ğŸ“‘ KnowLedge Distillation

We initially trained a highly specialized teacher model using the LLaMA 3.1-8B architecture, which demonstrated exceptional performance in the financial vertical domain. This model was meticulously fine-tuned to understand the nuances and complexities specific to financial data, allowing it to provide highly accurate predictions and insights in this specialized field. Following the successful training of the teacher model, we employed a knowledge distillation approach to transfer the knowledge from the LLaMA 3.1-8B model to a more compact LLaMA 2-7B model.

The distillation process, as illustrated in below, involved distilling knowledge from the M-layer teacher model (the original, unpruned model) into the N-layer student model (the pruned model). The student model was trained to mimic the behavior of the teacher model by minimizing a combination of losses, including the embedding output loss, the logit loss, and specific losses from the Transformer encoder that were mapped between the student blocks (S) and the teacher blocks (T). This comprehensive loss minimization strategy allowed the student model to learn effectively from the teacher model, capturing its essential features and decision-making patterns.

<a href="https://github.com/OD-FinLLM/" target="_blank"><img src="./web_demo/assets/kd.png" alt="OD-FinLLM" style="width: 80%; min-width: 350px; display: block; margin: auto;"></a>

Through this distillation process, we ensured that the LLaMA 2-7B model could maintain a high level of performance and accuracy in financial tasks, similar to that of its larger counterpart, while benefiting from reduced computational requirements and improved efficiency. This approach resulted in a robust and efficient financial model that leverages the strengths of both architectures, combining the deep understanding of the larger model with the practical advantages of the smaller model.



## Contributors

OD-FinLLM is developed by the AI team at Firefly.

## ğŸ‘ Acknowledgments

We acknowledge the following open-source projects and extend our gratitude to their respective developers:

- Facebook LLaMA: https://github.com/facebookresearch/llama
- LLaMA Factory: https://github.com/hiyouga/LLaMA-Factory

## Disclaimer

The resources provided by this project are strictly for academic research purposes and are not intended for commercial use. Please adhere to the corresponding open-source licenses when using any third-party code. The accuracy of the content generated by the model cannot be guaranteed due to factors such as computation, randomness, and quantization precision loss. The project assumes no legal responsibility for the content generated by the model, nor for any potential losses resulting from the use of these resources and outputs.

## TODO List

- [ ] Larger Model.
- [ ] Support multi task SFT in the Chinese financial field.
- [ ] CUDA deployment supporting quantitative models
- [ ] Reinforcement Learning -> Chat

## ğŸ“Œ Citation

If you use the data or code from this project, please cite it as follows:


## â˜ï¸ Contact and Feedback

We appreciate your support! Feel free to star ğŸŒŸ, watch, and share this project. For any issues, please submit them via GitHub Issues.
